{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Servian Logo](assets/servian_logo.png)\n",
    "# Creating KubeFlow Pipelines  - Lab #\n",
    "\n",
    "# Building a Kubeflow Pipeline #\n",
    "\n",
    "In this lab we will explore a dataset and turn exploratory analysis into a Kubeflow pipeline.\n",
    "\n",
    "Some of the key concepts that will be covered include:\n",
    "\n",
    "* Building lightweight Kubflow components from developed code.\n",
    "* Using a factory component\n",
    "* Accessing an external data sources.\n",
    "\n",
    "\n",
    "\n",
    "## Using Jupyter Notebooks as Development and Deployment Environment. ##\n",
    "\n",
    "Jupyter notebooks can be used for the entire development and and deploymeny lifecycle and offers the advantages of consumable source control and a single development and execution environment.\n",
    "\n",
    "Working exploratory code can be easily converted to Kubeflow components from within the notebook and deployed to a trainging environment.\n",
    "\n",
    "## Pipelines, Components and Tasks ##\n",
    "\n",
    "A pipeline is a collection of components that when chained together form the pipeline.  At is heart a component is simply code that takes inputs, applies logic and returns an output.  A task is a run of a pipeline component.\n",
    "\n",
    "Unlike a program executions a component is standalone and atomic and **does not** share, state, memory or resources with any other component.  A pipeline is a directed acyclic graph (DAG) of Kubeflow pipeline components.\n",
    "\n",
    "## Preparation ##\n",
    "\n",
    "* If the Kubeflow Pipelines Python SDK is not installed then it can be installed using pip or conda.\n",
    "* You will need to be within a Project running Kubeflow pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kfp==1.4\n",
    "#!pip install fire\n",
    "#!pip install pandas-gbq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detailed list of factory components can be viewed here: https://github.com/kubeflow/pipelines/tree/master/components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Kubeflow Pipeline in Python #\n",
    "\n",
    "What we will do:\n",
    "\n",
    "1. Build a Kubeflow pipeline with 2 tasks using a factory component\n",
    "1. Build a lightweight Python component to test the output of the tasks and add it to the pipeline\n",
    "1. Build a container based component to train a classification model and add it to the pipeline\n",
    "1. Add a factory component to the pipeline to submit a training job to Google Cloud AI Platform\n",
    "\n",
    "The execution graph looks like this:\n",
    "\n",
    "![DAG](assets/DAG.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Modules and Declaring Variables ##\n",
    "\n",
    "We need to import modules to help us build and compile Kubeflow pipelines. \n",
    "\n",
    "These will vary by pipeline but in this case we need the following:\n",
    "\n",
    "\n",
    "### Imports ###\n",
    "* The Kubeflow Pipelines SDK\n",
    "\n",
    "    ```import kfp```\n",
    "\n",
    "* kfp.dsl compiles the pipeline written in Python to KFPs domain specific language (DSL) which is expressed in YAML.\n",
    "\n",
    "    ```import kfp.dsl as dsl```\n",
    "\n",
    "* func_to_container_op turns a Python function into a container which is then used as a KFP component to build a pipeline.\n",
    "\n",
    "    ```from kfp.components import func_to_container_op```\n",
    "\n",
    "* ComponentStore allows us to search for pre written and compiled Kubeflow components to include in KFP pipelines.\n",
    "\n",
    "    ```from kfp.components import ComponentStore```\n",
    "\n",
    "* We are using Kubernetes secrets for authentication against GCP services so require use_gcp_secret\n",
    "\n",
    "    ```from kfp.gcp import use_gcp_secret```\n",
    "\n",
    "* KFP can be very senstitive to types passed between components, we need to explicitly type named tuples.\n",
    "\n",
    "    ```from typing import NamedTuple```\n",
    "\n",
    "\n",
    "### Variables ###\n",
    "\n",
    "* Location of component factory components - this is where the GCP factory components are.\n",
    "\n",
    "    ```COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/'```\n",
    "\n",
    "* GCP Project ID\n",
    "\n",
    "    ```PROJECT_ID = 'servian-gcp-training'```\n",
    "\n",
    "* Any Python lightweight component we create will need a base container iamge to be built from.\n",
    "\n",
    "    ```IMAGE_URI = 'gcr.io/deeplearning-platform-release/base-cpu'```\n",
    "    \n",
    "\n",
    "* As we are using the Python SDK to submit runs we weill need to specify the host name.\n",
    "\n",
    "    ```KFP_HOST = '656ef6603c3ce37d-dot-us-central2.pipelines.googleusercontent.com'```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to view available deep learning containers\n",
    "\n",
    "#!gcloud container images list \\\n",
    "#  --repository=\"gcr.io/deeplearning-platform-release\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubeflow pipelines\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.components import ComponentStore\n",
    "from kfp.gcp import use_gcp_secret\n",
    "from typing import NamedTuple\n",
    "\n",
    "# Compiler for demo below\n",
    "\n",
    "from kfp.compiler import Compiler\n",
    "\n",
    "\n",
    "\n",
    "# Location of component factory components - this is where the GCP factory components are\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/'\n",
    "\n",
    "# GCP Project ID\n",
    "PROJECT_ID = 'servian-gcp-training'\n",
    "\n",
    "# The container image we will use\n",
    "IMAGE_URI = 'gcr.io/deeplearning-platform-release/base-cpu'\n",
    "\n",
    "# Kubeflow pipelines hosty URI\n",
    "KFP_HOST = '6f81f68668e18bd7-dot-us-central1.pipelines.googleusercontent.com'\n",
    "\n",
    "# Region\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a Pipeline with Two Tasks Using a Factory Component ##\n",
    "\n",
    "Using a factory component to create a task is straighforward\n",
    "\n",
    "1. Declare where your factory components definitions are stored\n",
    "2. Load the component you want to use.\n",
    "\n",
    "In this example we are using the GCP BigQuery component and its is located at ```COMPONENT_URL_SEARCH_PREFIX``` declared above.\n",
    "\n",
    "This is a third-party component and reading the documentation is advised. The specific documentation is here:\n",
    "\n",
    "https://github.com/kubeflow/pipelines/tree/master/components/gcp/bigquery/query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factory Component Variables ###\n",
    "\n",
    "As per the documentation we can pass in variables, while many are optional its wortjh being explicit for anything where a random storage location is created.\n",
    "\n",
    "We need somewhere to store the CSV ouput\n",
    "\n",
    "```output_gcs_name```\n",
    "\n",
    "We need the location of a dataset to store the BQ table\n",
    "\n",
    "```dataset_location```\n",
    "\n",
    "```dataset_id```\n",
    "\n",
    "```table_id``` \n",
    "\n",
    "We need an SQL query to pass in\n",
    "\n",
    "```my_query```\n",
    "\n",
    "\n",
    "These are passed in as pipeline variables and consumed by the pipeline components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = !gcloud config get-value project\n",
    "project_id = project_id[0]\n",
    "project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GCS bucket URI where you will store files\n",
    "gcs_bucket_root = 'servian-gcp-training'\n",
    "my_things = 'jamie'\n",
    "\n",
    "output_gcs_name = 'gs://{}/labs/{}'.format(gcs_bucket_root,my_things)\n",
    "print(output_gcs_name)\n",
    "\n",
    "# Create component factory component (bigquery)\n",
    "component_store = ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "# Create BigQuery Operator\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% sample = keeping it small for the demo\n",
    "training_query = f'''SELECT\n",
    "                          *\n",
    "                     FROM\n",
    "                          `{project_id}.covertype.covertype` AS cover\n",
    "                     WHERE\n",
    "                          MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (0)'''\n",
    "\n",
    "\n",
    "# 20% sample\n",
    "testing_query = f'''SELECT\n",
    "                          *\n",
    "                     FROM\n",
    "                          `{project_id}.covertype.covertype` AS cover\n",
    "                     WHERE\n",
    "                          MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (8,9)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "df = pd.read_gbq(testing_query)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "bqclient = bigquery.Client()\n",
    "\n",
    "df = (\n",
    "    bqclient.query(testing_query)\n",
    "    .result()\n",
    "    .to_dataframe(create_bqstorage_client=True)\n",
    ")\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_table_id = '{}_train'.format(my_things)\n",
    "testing_table_id = '{}_test'.format(my_things)\n",
    "\n",
    "\n",
    "training_gcs_name = output_gcs_name+'/train.csv'\n",
    "testing_gcs_name = output_gcs_name+'/test.csv'\n",
    "\n",
    "print(training_gcs_name)\n",
    "print(testing_gcs_name)\n",
    "\n",
    "\n",
    "dataset_location = 'US'\n",
    "dataset_id = 'covertype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='{}-factory-component-pipeline'.format(my_things),\n",
    "    description='A pipeline using a factory component to carry out two tasks')\n",
    "\n",
    "def pipeline(project_id=PROJECT_ID,\n",
    "            region = REGION,\n",
    "            training_query=training_query,\n",
    "            training_table_id=training_table_id,\n",
    "            training_gcs_name=training_gcs_name,\n",
    "            testing_table_id=testing_table_id,\n",
    "            testing_gcs_name=testing_gcs_name, \n",
    "            dataset_location=dataset_location,\n",
    "            ):\n",
    "    \n",
    "    \n",
    "    # Get training data - save to BQ and GCS\n",
    "    get_training_data = bigquery_query_op(\n",
    "                  query=training_query,\n",
    "                  project_id=project_id,\n",
    "                  dataset_id=dataset_id,\n",
    "                  table_id=training_table_id,\n",
    "                  dataset_location=dataset_location,\n",
    "                  output_gcs_path=training_gcs_name)\n",
    "    \n",
    "    # Get training data - save to BQ and GCS\n",
    "    get_testing_data = bigquery_query_op(\n",
    "                  query=training_query,\n",
    "                  project_id=project_id,\n",
    "                  dataset_id=dataset_id,\n",
    "                  table_id=testing_table_id,\n",
    "                  dataset_location=dataset_location,\n",
    "                  output_gcs_path=testing_gcs_name).after(get_training_data)\n",
    "    \n",
    "    # Authentication to services for K8s service account\n",
    "    #kfp.dsl.get_pipeline_conf().add_op_transformer(use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a YAML / tgz file for upload ###\n",
    "\n",
    "Use the following URL to manually upload via the web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('https://{}/#/pipeline_versions/new'.format(KFP_HOST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{}.yaml'.format(my_things))\n",
    "\n",
    "Compiler().compile(pipeline, '{}.yaml'.format(my_things))\n",
    "# or\n",
    "Compiler().compile(pipeline, '{}.tgz'.format(my_things))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore yaml file\n",
    "#!cat jamie.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading via the Python Client ###\n",
    "\n",
    "This is our preferred way of doing this for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host=KFP_HOST)\n",
    "pipeline = client.create_run_from_pipeline_func(pipeline, experiment_name='Kubeflow Pipeline Labs', arguments={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Add Lightweight Python Component to the Pipeline #\n",
    "\n",
    "    \n",
    "In this step we will create a lightweight component which runs as a container and takes the ouput of the the biquery task and tests the output.  \n",
    "\n",
    "Lightweight Python components do not require you to build a new container image for every code change. They’re intended for fast iteration in a notebook environment.\n",
    "\n",
    "Advantages over container components:\n",
    "\n",
    "* Faster iteration: No need to build new container image after every change (building images takes some time).\n",
    "* Easier authoring: Components can be created in a local environment. Docker and Kubernetes are not required.\n",
    "\n",
    "\n",
    "To build a component, we define a stand-alone Python function and then call kfp.components.func_to_container_op(func) to convert the function to a component that can be used in a pipeline.\n",
    "\n",
    "There are several requirements for the component function:\n",
    "\n",
    "The function must be stand-alone.\n",
    "\n",
    "* It should not use any code declared outside the function definition.\n",
    "* Any imports should be added inside the main component function.\n",
    "* Any helper functions should also be defined inside the main component function.\n",
    "* The function can only import packages that are available in the base image.\n",
    "\n",
    "If you need to import a package that’s not available in the default base image you can try to find a container image that already includes the required packages. \n",
    "\n",
    "If the function operates on numbers, the parameters must have type hints. Supported types are int, float, bool. All other arguments are passed as strings.\n",
    "\n",
    "To build a component with multiple output values, use Python’s typing.NamedTuple type hint syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight Python component function\n",
    "def check_outputs(output) -> str:\n",
    "    print(type(output))\n",
    "    print(str(output))\n",
    "    return 'Success'\n",
    "\n",
    "check_outputs_operator = func_to_container_op(check_outputs,  base_image=IMAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_outputs('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='{}-factory-component-pipeline-w-lwc'.format(my_things),\n",
    "    description='A pipeline using a factory component to carry out two tasks and a lightweight component')\n",
    "\n",
    "def pipeline(project_id=PROJECT_ID,\n",
    "            region = REGION,\n",
    "            training_query=training_query,\n",
    "            training_table_id=training_table_id,\n",
    "            training_gcs_name=training_gcs_name,\n",
    "            testing_table_id=testing_table_id,\n",
    "            testing_gcs_name=testing_gcs_name, \n",
    "            dataset_location=dataset_location,\n",
    "            ):\n",
    "    \n",
    "    \n",
    "    # Get training data - save to BQ and GCS\n",
    "    get_training_data_task = bigquery_query_op(\n",
    "                  query=training_query,\n",
    "                  project_id=project_id,\n",
    "                  dataset_id=dataset_id,\n",
    "                  table_id=training_table_id,\n",
    "                  dataset_location=dataset_location,\n",
    "                  output_gcs_path=training_gcs_name)\n",
    "    \n",
    "    # Get testing data - save to BQ and GCS\n",
    "    get_testing_data_task = bigquery_query_op(\n",
    "                  query=training_query,\n",
    "                  project_id=project_id,\n",
    "                  dataset_id=dataset_id,\n",
    "                  table_id=testing_table_id,\n",
    "                  dataset_location=dataset_location,\n",
    "                  output_gcs_path=testing_gcs_name)\n",
    "    \n",
    "    # Check output of get training data task\n",
    "    check_output_task1 =  check_outputs_operator(get_training_data_task.outputs['output_gcs_path'])\n",
    "    \n",
    "    # Check output of get testing data task\n",
    "    check_output_task2 =  check_outputs_operator(get_testing_data_task.outputs['output_gcs_path'])\n",
    "    \n",
    "    # Authentication to services for K8s service account\n",
    "    #kfp.dsl.get_pipeline_conf().add_op_transformer(use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host=KFP_HOST)\n",
    "pipeline = client.create_run_from_pipeline_func(pipeline, experiment_name='Kubeflow Pipeline Labs', arguments={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a Container Component and Add it the the Pipeline ##\n",
    "\n",
    "For this we be actually running some training code.\n",
    "\n",
    "The great thing about this is you can simply take working notebook code and easily convert it to a full container component.\n",
    "\n",
    "You can still do exploration and local experimentation in the same notebook as pipelines are created.\n",
    "\n",
    "What we will be doing\n",
    "\n",
    "1. Create a training function\n",
    "1. Test the trainging function\n",
    "1. Create a container from the training function\n",
    "1. Deploy the container to the container registry\n",
    "1. Add the container component to the pipeline as a training task\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(training_gcs_name)\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create Your Training Function ###\n",
    "\n",
    "Here is a training function, you can test it locally to see if its working. It does the following:\n",
    "\n",
    "1. Reads trainging data from GCS\n",
    "1. Trains a model\n",
    "1. Saves the model locally and uploads it to GCS\n",
    "\n",
    "To get this function ready for deployment once we have tested it we need to do the following:\n",
    "\n",
    "1. Write the function to file\n",
    "1. Create a Dockerfile\n",
    "1. Build the container and submit to the Google Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dicrectory for the files \n",
    "\n",
    "!mkdir training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile training/train.py\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(job_dir, training_gcs_name, gcs_bucket_root, alpha, max_iter, model_dir, my_things):\n",
    "    \n",
    "   \n",
    "    df_train = pd.read_csv(training_gcs_name)\n",
    "    \n",
    "    numeric_feature_indexes = slice(0, 10)\n",
    "    categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "  ])\n",
    "    \n",
    "    num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "#     df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    X_train = df_train.drop('class', axis=1)\n",
    "    y_train = df_train['class']\n",
    "\n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    print('Model all trained....')\n",
    "    \n",
    "    # Upload model top GCS\n",
    "    print('Uploading model to GCS')\n",
    "    model_filename = 'model.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "    \n",
    "    gcs_model_path = '{}/{}'.format(model_dir, model_filename)\n",
    "    \n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(gcs_bucket_root)\n",
    "    blob = bucket.blob('{}/{}/{}/{}'.format('labs',my_things, model_dir, model_filename))\n",
    "\n",
    "    blob.upload_from_filename(model_filename)\n",
    "    \n",
    "    model_gcs_path = 'gs://{}/{}/{}/{}'.format(gcs_bucket_root,'labs',my_things, model_dir)\n",
    "\n",
    "    print('Exported to: {}'.format(model_gcs_path))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#    fire.Fire(train_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Notebook Unit Test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save model to\n",
    "job_dir = 'model'\n",
    "model_dir = 'model'\n",
    "training_gcs_name = \"gs://servian-gcp-training/labs/jamie/train.csv\"\n",
    "#\n",
    "alpha = .0001\n",
    "max_iter = 1000\n",
    "my_things = 'jamie'\n",
    "gcs_bucket_root = 'servian-gcp-training'\n",
    "\n",
    "\n",
    "train_model(job_dir, training_gcs_name, gcs_bucket_root, alpha, max_iter, model_dir, my_things)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create and build Container ###\n",
    "\n",
    "#### Create Dockerfile ####\n",
    "\n",
    "Create and write to the folder created above.\n",
    "\n",
    "Note we are installing Python packages not in the base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our base image\n",
    "print(IMAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /training\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Image and Register with Google Container Registry ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='training'\n",
    "TAG=my_things\n",
    "TRAIN_IMAGE_URI='\"gcr.io/{}/{}:{}\"'.format(PROJECT_ID, IMAGE_NAME, TAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TRAIN_IMAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture build_output\n",
    "!gcloud builds submit --timeout 15m --tag \"gcr.io/servian-gcp-training/training:jamie\" training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Add The Container Component to the Pipeline ###\n",
    "\n",
    "Also!!! We will submit it to Cloud AI Platform to do the training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AI platform factory components\n",
    "\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "\n",
    "# Not necessary under this scenario\n",
    "alpha = '.0001'\n",
    "max_iter = '1000'\n",
    "model_dir = 'model'\n",
    "\n",
    "# Training image\n",
    "trainer_image = TRAIN_IMAGE_URI\n",
    "\n",
    "# This is where we will create the job_directory\n",
    "print(output_gcs_name)\n",
    "\n",
    "\n",
    "# Deployment variables\n",
    "\n",
    "RUNTIME_VERSION = '1.14'\n",
    "PYTHON_VERSION = '3.5'\n",
    "model_id = 'test_{}'.format(my_things)\n",
    "version_id = '{}_v1'.format(my_things)\n",
    "replace_existing_version = 'True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gcs_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='{}-factory-component-pipeline-w-lwc'.format(my_things),\n",
    "    description='A pipeline using a factory component to carry out two tasks and a lightweight component')\n",
    "\n",
    "def pipeline(project_id=PROJECT_ID,\n",
    "            region = REGION,\n",
    "            training_query=training_query,\n",
    "            training_table_id=training_table_id,\n",
    "            training_gcs_name=training_gcs_name,\n",
    "            testing_table_id=testing_table_id,\n",
    "            testing_gcs_name=testing_gcs_name, \n",
    "            dataset_location=dataset_location,\n",
    "            ouput_gcs_name=output_gcs_name,\n",
    "            trainer_image=trainer_image,\n",
    "             replace_existing_version=replace_existing_version\n",
    "            ):\n",
    "    \n",
    "    \n",
    "    # Get training data - save to BQ and GCS\n",
    "    get_training_data_task = bigquery_query_op(\n",
    "                  query=training_query,\n",
    "                  project_id=project_id,\n",
    "                  dataset_id=dataset_id,\n",
    "                  table_id=training_table_id,\n",
    "                  dataset_location=dataset_location,\n",
    "                  output_gcs_path=training_gcs_name)\n",
    "    \n",
    "    # Get testing data - save to BQ and GCS\n",
    "    get_testing_data_task = bigquery_query_op(\n",
    "                  query=training_query,\n",
    "                  project_id=project_id,\n",
    "                  dataset_id=dataset_id,\n",
    "                  table_id=testing_table_id,\n",
    "                  dataset_location=dataset_location,\n",
    "                  output_gcs_path=testing_gcs_name)\n",
    "    \n",
    "    # Check output of get training data task\n",
    "    check_output_task1 =  check_outputs_operator(get_training_data_task.outputs['output_gcs_path'])\n",
    "    \n",
    "    # Check output of get testing data task\n",
    "    check_output_task2 =  check_outputs_operator(get_testing_data_task.outputs['output_gcs_path'])\n",
    "    \n",
    "    \n",
    "    # Train the model in cloud AI platform\n",
    "    \n",
    "    job_dir = '{}/{}'.format(output_gcs_name, model_dir)\n",
    "    \n",
    "    #print(job_dir)\n",
    "\n",
    "    train_args = ['--model_dir', model_dir,\n",
    "        '--training_gcs_name', get_training_data_task.outputs['output_gcs_path'],\n",
    "        '--gcs_bucket_root', gcs_bucket_root, \n",
    "        '--alpha', alpha, \n",
    "        '--max_iter', max_iter,\n",
    "        '--my_things', my_things\n",
    "    ]\n",
    "\n",
    "    train_model_task = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=trainer_image,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "    \n",
    "    check_output_task3 =  check_outputs_operator(train_model_task.outputs)\n",
    "\n",
    "    \n",
    "    \n",
    "    deploy_model_task = mlengine_deploy_op(\n",
    "        model_uri=train_model_task.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version).after(get_testing_data_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host=KFP_HOST)\n",
    "pipeline = client.create_run_from_pipeline_func(pipeline, experiment_name='Kubeflow Pipeline Labs', arguments={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Some Predictions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv(testing_gcs_name)\n",
    "df_test = df_test.sample(10)\n",
    "df_test = df_test.drop('class', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get JSON to test in WEB UI ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "test_json = {}\n",
    "test_json['instances'] = df_test.values.tolist()\n",
    "json.dumps(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit JSON for Prediction via Notebook ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "input_file = 'serving_instances.json'\n",
    "\n",
    "with open(input_file, 'w') as f:\n",
    "    for index, row in df_test.head(2).iterrows():\n",
    "        f.write(json.dumps(list(row.values)))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "--model \"test_jamie\" \\\n",
    "--version \"jamie_v1\" \\\n",
    "--json-instances 'serving_instances.json' \\\n",
    "--region 'global'"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
