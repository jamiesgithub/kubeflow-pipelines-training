{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Servian Logo](assets/servian_logo.png)\n",
    "# Creating KubeFlow Pipelines  - Lab #\n",
    "\n",
    "# Building a Kubeflow Pipeline #\n",
    "\n",
    "In this lab we will explore a dataset and turn exploratory analysis into a Kubeflow pipeline.\n",
    "\n",
    "Some of the key concepts that will be covered include:\n",
    "\n",
    "* Building lightweight Kubflow components from developed code.\n",
    "* Using a factory component\n",
    "* Accessing an external data sources.\n",
    "\n",
    "\n",
    "\n",
    "## Using Jupyter Notebooks as Development and Deployment Environment. ##\n",
    "\n",
    "Jupyter notebooks can be used for the entire development and and deploymeny lifecycle and offers the advantages of consumable source control and a single development and execution environment.\n",
    "\n",
    "Working exploratory code can be easily converted to Kubeflow components from within the notebook and deployed to a trainging environment.\n",
    "\n",
    "## Pipelines, Components and Tasks ##\n",
    "\n",
    "A pipeline is a collection of components that when chained together form the pipeline.  At is heart a component is simply code that takes inputs, applies logic and returns an output.  A task is a run of a pipeline component.\n",
    "\n",
    "Unlike a program executions a component is standalone and atomic and **does not** share, state, memory or resources with any other component.  A pipeline is a directed acyclic graph (DAG) of Kubeflow pipeline components.\n",
    "\n",
    "## Preparation ##\n",
    "\n",
    "* If the Kubeflow Pipelines Python SDK is not installed then it can be installed using pip or conda.\n",
    "* You will need to be within a Project running Kubeflow pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kfp\n",
    "#!pip install fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detailed list of factory components can be viewed here: https://github.com/kubeflow/pipelines/tree/master/components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Kubeflow Pipeline in Python #\n",
    "\n",
    "What we will do:\n",
    "\n",
    "1. Build a Kubeflow pipeline with 2 tasks using a factory component\n",
    "1. Build a lightweight Python component to test the output of the tasks and add it to the pipeline\n",
    "1. Build a container based component to train a classification model and add it to the pipeline\n",
    "1. Add a factory component to the pipeline to submit a training job to Google Cloud AI Platform\n",
    "\n",
    "The execution graph looks like this:\n",
    "\n",
    "![DAG](assets/DAG.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Modules and Declaring Variables ##\n",
    "\n",
    "We need to import modules to help us build and compile Kubeflow pipelines. \n",
    "\n",
    "These will vary by pipeline but in this case we need the following:\n",
    "\n",
    "\n",
    "### Imports ###\n",
    "* The Kubeflow Pipelines SDK\n",
    "\n",
    "    ```import kfp```\n",
    "\n",
    "* kfp.dsl compiles the pipeline written in Python to KFPs domain specific language (DSL) which is expressed in YAML.\n",
    "\n",
    "    ```import kfp.dsl as dsl```\n",
    "\n",
    "* func_to_container_op turns a Python function into a container which is then used as a KFP component to build a pipeline.\n",
    "\n",
    "    ```from kfp.components import func_to_container_op```\n",
    "\n",
    "* ComponentStore allows us to search for pre written and compiled Kubeflow components to include in KFP pipelines.\n",
    "\n",
    "    ```from kfp.components import ComponentStore```\n",
    "\n",
    "* We are using Kubernetes secrets for authentication against GCP services so require use_gcp_secret\n",
    "\n",
    "    ```from kfp.gcp import use_gcp_secret```\n",
    "\n",
    "* KFP can be very senstitive to types passed between components, we need to explicitly type named tuples.\n",
    "\n",
    "    ```from typing import NamedTuple```\n",
    "\n",
    "\n",
    "### Variables ###\n",
    "\n",
    "* Location of component factory components - this is where the GCP factory components are.\n",
    "\n",
    "    ```COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/'```\n",
    "\n",
    "* GCP Project ID\n",
    "\n",
    "    ```PROJECT_ID = 'servian-gcp-training'```\n",
    "\n",
    "* Any Python lightweight component we create will need a base container iamge to be built from.\n",
    "\n",
    "    ```IMAGE_URI = 'gcr.io/deeplearning-platform-release/base-cpu'```\n",
    "    \n",
    "\n",
    "* As we are using the Python SDK to submit runs we weill need to specify the host name.\n",
    "\n",
    "    ```KFP_HOST = '656ef6603c3ce37d-dot-us-central2.pipelines.googleusercontent.com'```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to view available deep learning containers\n",
    "\n",
    "#!gcloud container images list \\\n",
    "#  --repository=\"gcr.io/deeplearning-platform-release\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubeflow pipelines\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.components import ComponentStore\n",
    "from kfp.gcp import use_gcp_secret\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "\n",
    "# Location of component factory components - this is where the GCP factory components are\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/'\n",
    "\n",
    "# GCP Project ID\n",
    "PROJECT_ID = 'servian-gcp-training'\n",
    "\n",
    "# The container image we will use\n",
    "IMAGE_URI = 'gcr.io/deeplearning-platform-release/base-cpu'\n",
    "\n",
    "# Kubeflow pipelines hosty URI\n",
    "KFP_HOST = '5c4264ee40616903-dot-us-central2.pipelines.googleusercontent.com'\n",
    "\n",
    "# Region\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a Pipeline with Two Tasks Using a Factory Component ##\n",
    "\n",
    "Using a factory component to create a task is straighforward\n",
    "\n",
    "1. Declare where your factory components definitions are stored\n",
    "2. Load the component you want to use.\n",
    "\n",
    "In this example we are using the GCP BigQuery component and its is located at ```COMPONENT_URL_SEARCH_PREFIX``` declared above.\n",
    "\n",
    "This is a third-party component and reading the documentation is advised. The specific documentation is here:\n",
    "\n",
    "https://github.com/kubeflow/pipelines/tree/master/components/gcp/bigquery/query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factory Component Variables ###\n",
    "\n",
    "As per the documentation we can pass in variables, while many are optional its wortjh being explicit for anything where a random storage location is created.\n",
    "\n",
    "We need somewhere to store the CSV ouput\n",
    "\n",
    "```output_gcs_name```\n",
    "\n",
    "We need the location of a dataset to store the BQ table\n",
    "\n",
    "```dataset_location```\n",
    "\n",
    "```dataset_id```\n",
    "\n",
    "```table_id``` \n",
    "\n",
    "We need an SQL query to pass in\n",
    "\n",
    "```my_query```\n",
    "\n",
    "\n",
    "These are passed in as pipeline variables and consumed by the pipeline components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://servian-gcp-training/labs/jamie\n"
     ]
    }
   ],
   "source": [
    "# The GCS bucket URI where you will store files\n",
    "gcs_bucket_root = 'servian-gcp-training'\n",
    "my_things = 'jamie'\n",
    "\n",
    "output_gcs_name = 'gs://{}/labs/{}'.format(gcs_bucket_root,my_things)\n",
    "print(output_gcs_name)\n",
    "\n",
    "# Create component factory component (bigquery)\n",
    "component_store = ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "# Create BigQuery Operator\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% sample = keeping it small for the demo\n",
    "training_query = '''SELECT\n",
    "                          *\n",
    "                     FROM\n",
    "                          `servian-gcp-training.covertype.covertype` AS cover\n",
    "                     WHERE\n",
    "                          MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (0)'''\n",
    "\n",
    "\n",
    "# 20% sample\n",
    "testing_query = '''SELECT\n",
    "                          *\n",
    "                     FROM\n",
    "                          `servian-gcp-training.covertype.covertype` AS cover\n",
    "                     WHERE\n",
    "                          MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (8,9)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://servian-gcp-training/labs/jamie/train.csv\n",
      "gs://servian-gcp-training/labs/jamie/test.csv\n"
     ]
    }
   ],
   "source": [
    "training_table_id = '{}_train'.format(my_things)\n",
    "testing_table_id = '{}_test'.format(my_things)\n",
    "\n",
    "\n",
    "training_gcs_name = output_gcs_name+'/train.csv'\n",
    "testing_gcs_name = output_gcs_name+'/test.csv'\n",
    "\n",
    "print(training_gcs_name)\n",
    "print(testing_gcs_name)\n",
    "\n",
    "\n",
    "dataset_location = 'US'\n",
    "dataset_id = 'covertype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='{}-factory-component-pipeline'.format(my_things),\n",
    "    description='A pipeline using a factory component to carry out two tasks')\n",
    "\n",
    "def pipeline(project_id=PROJECT_ID,\n",
    "            region = REGION,\n",
    "            training_query=training_query,\n",
    "            training_table_id=training_table_id,\n",
    "            training_gcs_name=training_gcs_name,\n",
    "            testing_table_id=testing_table_id,\n",
    "            testing_gcs_name=testing_gcs_name, \n",
    "            dataset_location=dataset_location,\n",
    "            ):\n",
    "    \n",
    "    \n",
    "    # Get training data - save to BQ and GCS\n",
    "    get_training_data = bigquery_query_op(\n",
    "                  query=training_query,\n",
    "                  project_id=project_id,\n",
    "                  dataset_id=dataset_id,\n",
    "                  table_id=training_table_id,\n",
    "                  dataset_location=dataset_location,\n",
    "                  output_gcs_path=training_gcs_name)\n",
    "    \n",
    "    # Get training data - save to BQ and GCS\n",
    "    get_testing_data = bigquery_query_op(\n",
    "                  query=training_query,\n",
    "                  project_id=project_id,\n",
    "                  dataset_id=dataset_id,\n",
    "                  table_id=testing_table_id,\n",
    "                  dataset_location=dataset_location,\n",
    "                  output_gcs_path=testing_gcs_name).after(get_training_data)\n",
    "    \n",
    "    # Authentication to services for K8s service account\n",
    "    #kfp.dsl.get_pipeline_conf().add_op_transformer(use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://5c4264ee40616903-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/057c5d0d-fee7-4ca6-97fe-d6ba7a8964b3\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://5c4264ee40616903-dot-us-central2.pipelines.googleusercontent.com/#/runs/details/90039f50-9c36-492f-af84-943b564c746e\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client(host=KFP_HOST)\n",
    "pipeline = client.create_run_from_pipeline_func(pipeline, experiment_name='Kubeflow Pipeline Labs', arguments={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Add Lightweight Python Component to the Pipeline #\n",
    "\n",
    "    \n",
    "In this step we will create a lightweight component which runs as a container and takes the ouput of the the biquery task and tests the output.  \n",
    "\n",
    "Lightweight Python components do not require you to build a new container image for every code change. They’re intended for fast iteration in a notebook environment.\n",
    "\n",
    "Advantages over container components:\n",
    "\n",
    "* Faster iteration: No need to build new container image after every change (building images takes some time).\n",
    "* Easier authoring: Components can be created in a local environment. Docker and Kubernetes are not required.\n",
    "\n",
    "\n",
    "To build a component, we define a stand-alone Python function and then call kfp.components.func_to_container_op(func) to convert the function to a component that can be used in a pipeline.\n",
    "\n",
    "There are several requirements for the component function:\n",
    "\n",
    "The function must be stand-alone.\n",
    "\n",
    "* It should not use any code declared outside the function definition.\n",
    "* Any imports should be added inside the main component function.\n",
    "* Any helper functions should also be defined inside the main component function.\n",
    "* The function can only import packages that are available in the base image.\n",
    "\n",
    "If you need to import a package that’s not available in the default base image you can try to find a container image that already includes the required packages. \n",
    "\n",
    "If the function operates on numbers, the parameters must have type hints. Supported types are int, float, bool. All other arguments are passed as strings.\n",
    "\n",
    "To build a component with multiple output values, use Python’s typing.NamedTuple type hint syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight Python component function\n",
    "def check_outputs(output) -> str:\n",
    "    print(type(output))\n",
    "    print(str(output))\n",
    "    return 'Success'\n",
    "\n",
    "check_outputs_operator = func_to_container_op(check_outputs,  base_image=IMAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='{}-factory-component-pipeline-w-lwc'.format(my_things),\n",
    "    description='A pipeline using a factory component to carry out two tasks and a lightweight component')\n",
    "\n",
    "def pipeline(project_id=PROJECT_ID,\n",
    "            region = REGION,\n",
    "            training_query=training_query,\n",
    "            training_table_id=training_table_id,\n",
    "            training_gcs_name=training_gcs_name,\n",
    "            testing_table_id=testing_table_id,\n",
    "            testing_gcs_name=testing_gcs_name, \n",
    "            dataset_location=dataset_location,\n",
    "            ):\n",
    "    \n",
    "    \n",
    "    # Get training data - save to BQ and GCS\n",
    "    get_training_data_task = bigquery_query_op(\n",
    "                  query=training_query,\n",
    "                  project_id=project_id,\n",
    "                  dataset_id=dataset_id,\n",
    "                  table_id=training_table_id,\n",
    "                  dataset_location=dataset_location,\n",
    "                  output_gcs_path=training_gcs_name)\n",
    "    \n",
    "    # Get testing data - save to BQ and GCS\n",
    "    get_testing_data_task = bigquery_query_op(\n",
    "                  query=training_query,\n",
    "                  project_id=project_id,\n",
    "                  dataset_id=dataset_id,\n",
    "                  table_id=testing_table_id,\n",
    "                  dataset_location=dataset_location,\n",
    "                  output_gcs_path=testing_gcs_name)\n",
    "    \n",
    "    # Check output of get training data task\n",
    "    check_output_task1 =  check_outputs_operator(get_training_data_task.outputs['output_gcs_path'])\n",
    "    \n",
    "    # Check output of get testing data task\n",
    "    check_output_task2 =  check_outputs_operator(get_testing_data_task.outputs['output_gcs_path'])\n",
    "    \n",
    "    # Authentication to services for K8s service account\n",
    "    #kfp.dsl.get_pipeline_conf().add_op_transformer(use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://5c4264ee40616903-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/057c5d0d-fee7-4ca6-97fe-d6ba7a8964b3\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://5c4264ee40616903-dot-us-central2.pipelines.googleusercontent.com/#/runs/details/39978109-ab83-465a-bf3e-1d4dfd5dbe87\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client(host=KFP_HOST)\n",
    "pipeline = client.create_run_from_pipeline_func(pipeline, experiment_name='Kubeflow Pipeline Labs', arguments={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Some key differences to the previous lightweight Python component we built are:\n",
    "\n",
    "* We want to be explicit about what we returm so we use ```NamedTuple``` from the Python ```typing``` library imported above.\n",
    "* We also format the return specifically as a named tuple using ```namedtuple``` from the Python ```collections``` library.\n",
    "* As we are not using standard Python we must import required libraries within the function. In this case ```pandas``` and ```namedtuple``` from ```collections```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a Container Component and Add it the the Pipeline ##\n",
    "\n",
    "For this we be actually running some training code.\n",
    "\n",
    "The great thing about this is you can simply take working notebook code and easily convert it to a full container component.\n",
    "\n",
    "You can still do exploration and local experimentation in the same notebook as pipelines are created.\n",
    "\n",
    "What we will be doing\n",
    "\n",
    "1. Create a training function\n",
    "1. Test the trainging function\n",
    "1. Create a container from the training function\n",
    "1. Deploy the container to the container registry\n",
    "1. Add the container component to the pipeline as a training task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(training_gcs_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area</th>\n",
       "      <th>Soil_Type</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2332</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>384</td>\n",
       "      <td>219</td>\n",
       "      <td>237</td>\n",
       "      <td>155</td>\n",
       "      <td>1328</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2702</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2637</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>390</td>\n",
       "      <td>211</td>\n",
       "      <td>2078</td>\n",
       "      <td>219</td>\n",
       "      <td>238</td>\n",
       "      <td>156</td>\n",
       "      <td>443</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2552</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>484</td>\n",
       "      <td>33</td>\n",
       "      <td>953</td>\n",
       "      <td>218</td>\n",
       "      <td>238</td>\n",
       "      <td>156</td>\n",
       "      <td>1044</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2563</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>417</td>\n",
       "      <td>21</td>\n",
       "      <td>1120</td>\n",
       "      <td>218</td>\n",
       "      <td>237</td>\n",
       "      <td>156</td>\n",
       "      <td>1262</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2559</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>16</td>\n",
       "      <td>1113</td>\n",
       "      <td>218</td>\n",
       "      <td>238</td>\n",
       "      <td>156</td>\n",
       "      <td>1332</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0       2332      90      0                                30   \n",
       "1       2637     135      0                               390   \n",
       "2       2552       0      0                               484   \n",
       "3       2563      45      0                               417   \n",
       "4       2559       0      0                               510   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                               0                              384   \n",
       "1                             211                             2078   \n",
       "2                              33                              953   \n",
       "3                              21                             1120   \n",
       "4                              16                             1113   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0            219             237            155   \n",
       "1            219             238            156   \n",
       "2            218             238            156   \n",
       "3            218             237            156   \n",
       "4            218             238            156   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points Wilderness_Area Soil_Type  Cover_Type  \n",
       "0                                1328           Cache     C2702           5  \n",
       "1                                 443       Commanche     C2703           2  \n",
       "2                                1044       Commanche     C2703           2  \n",
       "3                                1262       Commanche     C2703           2  \n",
       "4                                1332       Commanche     C2703           2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create Your Training Function ###\n",
    "\n",
    "Here is a training function, you can test it locally to see if its working. It does the following:\n",
    "\n",
    "1. Reads trainging data from GCS\n",
    "1. Trains a model\n",
    "1. Saves the model locally and uploads it to GCS\n",
    "\n",
    "To get this function ready for deployment once we have tested it we need to do the following:\n",
    "\n",
    "1. Write the function to file\n",
    "1. Create a Dockerfile\n",
    "1. Build the container and submit to the Google Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘training’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Create a dicrectory for the files \n",
    "\n",
    "!mkdir training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/train.py\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(job_dir, training_gcs_name, gcs_bucket_root, alpha, max_iter, model_dir, my_things):\n",
    "    \n",
    "   \n",
    "    df_train = pd.read_csv(training_gcs_name)\n",
    "    \n",
    "    numeric_feature_indexes = slice(0, 10)\n",
    "    categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "  ])\n",
    "    \n",
    "    num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "#     df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    X_train = df_train.drop('Cover_Type', axis=1)\n",
    "    y_train = df_train['Cover_Type']\n",
    "\n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    print('Model all trained....')\n",
    "    \n",
    "    # Upload model top GCS\n",
    "    print('Uploading model to GCS')\n",
    "    model_filename = 'model.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "    \n",
    "    gcs_model_path = '{}/{}'.format(model_dir, model_filename)\n",
    "    \n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(gcs_bucket_root)\n",
    "    blob = bucket.blob('{}/{}/{}/{}'.format('labs',my_things, model_dir, model_filename))\n",
    "\n",
    "    blob.upload_from_filename(model_filename)\n",
    "    \n",
    "    model_gcs_path = 'gs://{}/{}/{}/{}'.format(gcs_bucket_root,'labs',my_things, model_dir)\n",
    "\n",
    "    print('Exported to: {}'.format(model_gcs_path))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Notebook Unit Test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save model to\n",
    "job_dir = 'model'\n",
    "model_dir = 'model'\n",
    "training_gcs_name = \"gs://servian-gcp-training/labs/jamie/train.csv\"\n",
    "#\n",
    "alpha = .0001\n",
    "max_iter = 1000\n",
    "my_things = 'jamie'\n",
    "gcs_bucket_root = 'servian-gcp-training'\n",
    "\n",
    "\n",
    "train_model(job_dir, training_gcs_name, gcs_bucket_root, alpha, max_iter, model_dir, my_things)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create and build Container ###\n",
    "\n",
    "#### Create Dockerfile ####\n",
    "\n",
    "Create and write to the folder created above.\n",
    "\n",
    "Note we are installing Python packages not in the base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/deeplearning-platform-release/base-cpu\n"
     ]
    }
   ],
   "source": [
    "# Our base image\n",
    "print(IMAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /training\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile  train.py\n"
     ]
    }
   ],
   "source": [
    "ls training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Image and Register with Google Container Registry ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='training'\n",
    "TAG=my_things\n",
    "TRAIN_IMAGE_URI='\"gcr.io/{}/{}:{}\"'.format(PROJECT_ID, IMAGE_NAME, TAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"gcr.io/servian-gcp-training/training:jamie\"\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_IMAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture build_output\n",
    "!gcloud builds submit --timeout 15m --tag \"gcr.io/servian-gcp-training/training:jamie\" training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Add The Container Component to the Pipeline ###\n",
    "\n",
    "Also!!! We will submit it to Cloud AI Platform to do the training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://servian-gcp-training/labs/jamie\n"
     ]
    }
   ],
   "source": [
    "# Create the AI platform factory components\n",
    "\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "\n",
    "# Not necessary under this scenario\n",
    "alpha = '.0001'\n",
    "max_iter = '1000'\n",
    "model_dir = 'model'\n",
    "\n",
    "# Training image\n",
    "trainer_image = TRAIN_IMAGE_URI\n",
    "\n",
    "# This is where we will create the job_directory\n",
    "print(output_gcs_name)\n",
    "\n",
    "\n",
    "# Deployment variables\n",
    "\n",
    "RUNTIME_VERSION = '1.14'\n",
    "PYTHON_VERSION = '3.5'\n",
    "model_id = '{}'.format(my_things)\n",
    "version_id = '{}_v1'.format(my_things)\n",
    "replace_existing_version = 'True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://servian-gcp-training/labs/jamie'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_gcs_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='{}-factory-component-pipeline-w-lwc'.format(my_things),\n",
    "    description='A pipeline using a factory component to carry out two tasks and a lightweight component')\n",
    "\n",
    "def pipeline(project_id=PROJECT_ID,\n",
    "            region = REGION,\n",
    "            training_query=training_query,\n",
    "            training_table_id=training_table_id,\n",
    "            training_gcs_name=training_gcs_name,\n",
    "            testing_table_id=testing_table_id,\n",
    "            testing_gcs_name=testing_gcs_name, \n",
    "            dataset_location=dataset_location,\n",
    "            ouput_gcs_name=output_gcs_name,\n",
    "            trainer_image=trainer_image,\n",
    "             replace_existing_version=replace_existing_version\n",
    "            ):\n",
    "    \n",
    "    \n",
    "    # Get training data - save to BQ and GCS\n",
    "    get_training_data_task = bigquery_query_op(\n",
    "                  query=training_query,\n",
    "                  project_id=project_id,\n",
    "                  dataset_id=dataset_id,\n",
    "                  table_id=training_table_id,\n",
    "                  dataset_location=dataset_location,\n",
    "                  output_gcs_path=training_gcs_name)\n",
    "    \n",
    "    # Get testing data - save to BQ and GCS\n",
    "    get_testing_data_task = bigquery_query_op(\n",
    "                  query=training_query,\n",
    "                  project_id=project_id,\n",
    "                  dataset_id=dataset_id,\n",
    "                  table_id=testing_table_id,\n",
    "                  dataset_location=dataset_location,\n",
    "                  output_gcs_path=testing_gcs_name)\n",
    "    \n",
    "    # Check output of get training data task\n",
    "    check_output_task1 =  check_outputs_operator(get_training_data_task.outputs['output_gcs_path'])\n",
    "    \n",
    "    # Check output of get testing data task\n",
    "    check_output_task2 =  check_outputs_operator(get_testing_data_task.outputs['output_gcs_path'])\n",
    "    \n",
    "    \n",
    "    # Train the model in cloud AI platform\n",
    "    \n",
    "    job_dir = '{}/{}'.format(output_gcs_name, model_dir)\n",
    "    \n",
    "    #print(job_dir)\n",
    "\n",
    "    train_args = ['--model_dir', model_dir,\n",
    "        '--training_gcs_name', get_training_data_task.outputs['output_gcs_path'],\n",
    "        '--gcs_bucket_root', gcs_bucket_root, \n",
    "        '--alpha', alpha, \n",
    "        '--max_iter', max_iter,\n",
    "        '--my_things', my_things\n",
    "    ]\n",
    "\n",
    "    train_model_task = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=trainer_image,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "    \n",
    "    check_output_task3 =  check_outputs_operator(train_model_task.outputs)\n",
    "\n",
    "    \n",
    "    \n",
    "    deploy_model_task = mlengine_deploy_op(\n",
    "        model_uri=train_model_task.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version).after(get_testing_data_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"JsonObject\" based on the value \"{'job_id': {{pipelineparam:op=Submitting a Cloud ML training job as a pipeline step;name=job_id}}, 'job_dir': {{pipelineparam:op=Submitting a Cloud ML training job as a pipeline step;name=job_dir}}}\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://5c4264ee40616903-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/057c5d0d-fee7-4ca6-97fe-d6ba7a8964b3\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://5c4264ee40616903-dot-us-central2.pipelines.googleusercontent.com/#/runs/details/9ffef6fe-c033-4e71-930e-82f6e57e1187\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client(host=KFP_HOST)\n",
    "pipeline = client.create_run_from_pipeline_func(pipeline, experiment_name='Kubeflow Pipeline Labs', arguments={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Some Predictions ##"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
